# -*- coding: utf-8 -*-

import glob
from astropy.io import fits
import numpy as np

SAVE_MODEL=True

pathin='/home/fernando/mergers/simple_network/major_mergers_30_final/merg/'
merg_images = glob.glob(pathin+'/*.fits')
merg_images.sort()

cat_list = []
cat_path = '/home/fernando/mergers/simple_network/major_mergers_30_final/final_merger_cat_major_mergers.txt'
arch = open(cat_path)
lines = arch.readlines()[1:]
for line in lines:
    w = line.split(',')
    id_gal = w[2].strip()
    mass_ratio = float(w[10])
    cat_list.append((id_gal, mass_ratio))

def get_mr_from_id(merg_id):
    print merg_id,
    new_str = merg_id + '_'
    for id_gal, massratio in cat_list:
        new_str = id_gal + '_'
        if new_str in merg_id:
           return massratio
    return None
           
maxim=200;
D=np.zeros([maxim+1,1,128,128])
Y=np.zeros([maxim+1,1])
iteri=-1;
numim=0;

iteri = 0
numim = 0
while numim < 200:
    thepath = merg_images[iteri]
    theid = thepath.split('/')[-1]
    hdulist = fits.open(thepath)
    iteri=iteri+1
    scidata1 = hdulist[0].data
    hdulist.close()
    if scidata1.shape[1] > 128: continue
    D[numim,:,:,:]=scidata1;
    themr = get_mr_from_id(theid)
    Y[numim]=themr
    numim+=1
    print "/ mass ratio", themr, "(n=%d)" % (numim)
    Y = Y.squeeze()

X_train = D[0:len(D)//5*4,0,:,:]   
X_val = D[len(D)//5*4:,0,:,:]
Y_train = Y[0:len(D)//5*4]
Y_val = Y[len(D)//5*4:]
print (X_train.shape)
print (X_val.shape)
print (Y_train.shape)
print (Y_val.shape)

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.optimizers import SGD

# model params
batch_size = 64
nb_epoch = 10
data_augmentation = False
normalize = False
y_normalization = True
norm_constant = 255 

# SGD parameters
lr=0.01   #0.01
decay=0   #1e-6
momentum=0.9   #0.9
nesterov=True

depth=16
nb_dense = 64

#output params
verbose = 1

## Test identification
test_name = "batch_size=%d-nb_epoch=%d-augm=%s-lr=%f-decay=%f-mom=%f-depth=%d-nb_dense=%d"%(batch_size, nb_epoch, data_augmentation, lr, decay, momentum, depth, nb_dense)
print("Test name is: " + test_name)

# input image dimensions
img_rows, img_cols = X_train.shape[1:3]
img_channels = 1
print(img_rows, img_cols)

### Right shape for X
X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)
X_val = X_val.reshape(X_val.shape[0], 1, img_rows, img_cols)

model = Sequential()
model.add(Convolution2D(depth, 3, 3, border_mode='same',
                        input_shape=(img_channels, img_rows, img_cols)))
model.add(Activation('relu'))
model.add(Convolution2D(depth, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Convolution2D(2*depth, 3, 3, border_mode='same'))
model.add(Activation('relu'))
model.add(Convolution2D(2*depth, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(nb_dense))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))


# let's train the model using SGD + momentum (how original).
sgd = SGD(lr=lr, decay=decay, momentum=momentum, nesterov=True)
model.compile(loss='mean_absolute_error',
              optimizer=sgd)

import os
weights_file_name = test_name+".hd5"
if os.path.isfile(weights_file_name) and SAVE_MODEL:
    print("Model has already been computed. Loading it.")
    model.load_weights(weights_file_name)
else:
    if not data_augmentation:
        print('Not using data augmentation.')
        history = model.fit(X_train, Y_train,
                  batch_size=batch_size,
                  nb_epoch=nb_epoch,
                  validation_data=(X_val, Y_val),
                  shuffle=True,
                  verbose=verbose)
    else:
        print('Using real-time data augmentation.')

        # this will do preprocessing and realtime data augmentation
        datagen = ImageDataGenerator(
            featurewise_center=False, 
            samplewise_center=False, 
            featurewise_std_normalization=False, 
            samplewise_std_normalization=False,
            zca_whitening=False, 
            rotation_range=0, 
            width_shift_range=0.0,  
            height_shift_range=0.0, 
            horizontal_flip=True,
            vertical_flip=True)  

        # compute quantities required for featurewise normalization
        # (std, mean, and principal components if ZCA whitening is applied)
        datagen.fit(X_train)

        # fit the model on the batches generated by datagen.flow()
        history = model.fit_generator(datagen.flow(X_train, Y_train,
                            batch_size=batch_size),
                            samples_per_epoch=X_train.shape[0],
                            nb_epoch=nb_epoch,
                            validation_data=(X_val, Y_val),
                            verbose=verbose)

    # save model
    if SAVE_MODEL:
        print("Saving model...")
        model.save_weights(weights_file_name)

print("Best validation loss: %.3f" % (np.min(history.history['val_loss'])))
print("at: %d" % np.argmin(history.history['val_loss']))

# Printing some stuff
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

fig = plt.figure()
plt.plot(history.epoch, history.history['loss'], label='loss')
plt.plot(history.epoch, history.history['val_loss'], label='val_loss')
plt.title('Training performance')
plt.legend()
plt.savefig('TrainingPerformance.png')

Y_pred = model.predict(X_val)
plt.figure(figsize=(12,12))
plt.scatter(Y_val, Y_pred)
plt.legend()
plt.savefig('ScatterPlot.png')

